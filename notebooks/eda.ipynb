{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7b7c76c",
   "metadata": {},
   "source": [
    "## EDA for Credit Card Fraud Detection System (FDS)\n",
    "##### Gavin Qu, version 11.14.2025"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc7a694",
   "metadata": {},
   "source": [
    "#### Content: \n",
    "- The dataset contains transactions made by credit cards in September 2013 by European cardholders.\n",
    "- This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions. \n",
    "\n",
    "- It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-sensitive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise. \n",
    "\n",
    "- Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification. \n",
    "\n",
    "- *PCA*: The idea is to project the data onto a new set of orthogonal axes, called PCs ordered so that the first PC accounts for the largest variance in the data, the second PC account for the largest remaining variance orthogonal to the first, and so on. By keeping only the first few PCs (which captures most variance), you can simplify the data and reduce noise without losing the essential relationships between data. \n",
    "- The eigenvectors of the covariance matrix is the principle components\n",
    "- The eigenvector with the highest eigenvalue is the first PC, the second largest is PC2. \n",
    "- When you discard the principle components that explain the least variance (p - k components), you are also discarding the directions that contain the most \"noise\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3159a539",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "# path = kagglehub.dataset_download(\"mlg-ulb/creditcardfraud\")\n",
    "\n",
    "#print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139db411",
   "metadata": {},
   "source": [
    "### 1. Objective and Constraints\n",
    "#### 1.1 Loss of Interpretability and Gain in Dimensionality Reduction\n",
    "- Since PCA creates new axes that are linear combination of the original features, the column headers V1, V2, etc. no longer directly correspond to interpretable real world entities (like age, ZIP code...)\n",
    "- Given the loss of correlation to original features, the coefficients used are unknown, which limits our ability to perform direct, causal analysis based on feature names alone. \n",
    "- Since PCs are orthogonal and uncorrelated to each other, this is actually beneficial for my usage of logistics regression, as it eliminates the issue of multicollinearity. \n",
    "- Features created this way should be ranked by amount of the variance they capture, V1 captures the most information (variance) in the original dataset and V28 captures the least information. This gives us the possibility of further dimensionality reduction if we are to drop the last features. \n",
    "- Since the original dataset has to be scaled/normalized before the PCA, and the PCs are derived from the covariance matrix, the original features scales are mixed together in the PCs. As a result, I don't have to worry about the scale of the features relative to each other from V1-V28. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e086f3c",
   "metadata": {},
   "source": [
    "#### 1.2 Objective Formulation\n",
    "I consider it success at this stage as gaining a robust understanding of the dataset and finding a mixture of the models that best predict the outcome using the given principal components. Speed is not too much of a concern but I'd like to simulate real-world scenarios in consideration of model development cycles. Given the class imbalance, the key metric to measure performance should be Area under the Precision-Recall Curve (AUPRC). \n",
    "1. Do \"Time\", \"Amount\" and \"Class\" show clustering based on other features? This could introduce leakage. \n",
    "2. How do features V1-V28 behave distributionally across our predicted variable Class? \n",
    "3. Time-series considerations. \n",
    "4. What should I do with class imbalance ratio? Do I need to interpret results differently from other types of datasets? e.g. summary statistics\n",
    "5. How do we interpret the V1-V28 Principle Components if we find feature importance? \n",
    "6. Most importantly, where does the model learn to separate signal from noise? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
